{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "426.667px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "09. Text_Analysis.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unknown-jun/Python_ML_Guide_Book/blob/main/09_Text_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d381ce53"
      },
      "source": [
        "# 텍스트 분석\n",
        "## 텍스트 사전 준비 작업(텍스트 전처리)-텍스트 정규화\n",
        "### 텍스트 토큰화"
      ],
      "id": "d381ce53"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6a0d068",
        "outputId": "fa8a4a8e-3f6d-4239-f824-07ce253208fa"
      },
      "source": [
        "from nltk import sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "text_sample = 'The Matrix is everwhere its all around us, here even in this room. \\\n",
        "               You cam see it out your window or on your television. \\\n",
        "               You feel it when you go to work, or go to church or pay your taxes.'\n",
        "\n",
        "sentences = sent_tokenize(text = text_sample)\n",
        "print(type(sentences), len(sentences))\n",
        "print(sentences)"
      ],
      "id": "a6a0d068",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'> 3\n",
            "['The Matrix is everwhere its all around us, here even in this room.', 'You cam see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Jun\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb70d52f"
      },
      "source": [
        "### 단어 토큰화"
      ],
      "id": "cb70d52f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c9702a2",
        "outputId": "8cc03c49-5277-41f8-cc04-908d22699511"
      },
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "sentence = \"The Matrix is everwhere its all around us, here even in this room.\"\n",
        "words = word_tokenize(sentence)\n",
        "print(type(words), len(words))\n",
        "print(words)"
      ],
      "id": "1c9702a2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'> 15\n",
            "['The', 'Matrix', 'is', 'everwhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba63405d",
        "outputId": "726c6467-cadd-494c-cd50-77aaf99e4a84"
      },
      "source": [
        "from nltk import word_tokenize, sent_tokenize\n",
        "\n",
        "# 여러 개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화하게 만드는 함수 생성\n",
        "def tokenize_text(text):\n",
        "    \n",
        "    # 문장별로 분리 토큰\n",
        "    sentences = sent_tokenize(text)\n",
        "    # 분리된 문장별 단어 토큰화\n",
        "    word_tokens = [ word_tokenize(sentence) for sentence in sentences ]\n",
        "    \n",
        "    return word_tokens\n",
        "\n",
        "# 여러 문장에 대해 문장별 단어 토큰화 수행.\n",
        "word_tokens = tokenize_text(text_sample)\n",
        "print(type(word_tokens), len(word_tokens))\n",
        "print(word_tokens)"
      ],
      "id": "ba63405d",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'> 3\n",
            "[['The', 'Matrix', 'is', 'everwhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'cam', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9990922a"
      },
      "source": [
        "### 스톱 워드 제거"
      ],
      "id": "9990922a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6c33cdf",
        "outputId": "894f0916-194f-4e26-845e-385f16e9b4e3"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "id": "a6c33cdf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Jun\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f456115c",
        "outputId": "247e8ef7-0b36-4df4-c729-448c478c39e5"
      },
      "source": [
        "print('영어 stop words 개수: ', len(nltk.corpus.stopwords.words('english')))\n",
        "print(nltk.corpus.stopwords.words('english')[:20])"
      ],
      "id": "f456115c",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "영어 stop words 개수:  179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccd5f2cf",
        "outputId": "ce297b26-7568-42f0-8ad7-0ab3f98016f3"
      },
      "source": [
        "import nltk\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "all_tokens = []\n",
        "# 위 예제에서 3개의 문장별로 얻은 word_tokens_list에 대해 스톱 워드를 제거하는 반복문\n",
        "for sentence in word_tokens:\n",
        "    filtered_words = [ ]\n",
        "    # 개별 문장별로 토큰화된 문장 list에 대해 스톱 워드를 제거하는 반복문\n",
        "    for word in sentence:\n",
        "        # 소문자로 모두 변환\n",
        "        word = word.lower( )\n",
        "        # 토큰화된 개별 단어가 스톱 워드의 단어에 포함되지 않으면 word_tokens에 추가\n",
        "        if word not in stopwords:\n",
        "            filtered_words.append(word)\n",
        "    all_tokens.append(filtered_words)\n",
        "\n",
        "print(all_tokens)"
      ],
      "id": "ccd5f2cf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['matrix', 'everwhere', 'around', 'us', ',', 'even', 'room', '.'], ['cam', 'see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7a4dfee"
      },
      "source": [
        "### Stemming과 Lemmatization"
      ],
      "id": "c7a4dfee"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fd88d11",
        "outputId": "5d96ac10-6695-4491-9ac8-69fca7fc8788"
      },
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "stemmer = LancasterStemmer( )\n",
        "\n",
        "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
        "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
        "print(stemmer.stem('happier'), stemmer.stem('happiest'))\n",
        "print(stemmer.stem('fancier'), stemmer.stem('fanciest'))"
      ],
      "id": "9fd88d11",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "work work work\n",
            "amus amus amus\n",
            "happy happiest\n",
            "fant fanciest\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f92e781",
        "outputId": "b59582e3-aa3d-4b30-d836-4554412bd300"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemma = WordNetLemmatizer( )\n",
        "print(lemma.lemmatize('amusing', 'v'), lemma.lemmatize('amuses', 'v'), lemma.lemmatize('amused', 'v'))\n",
        "print(lemma.lemmatize('happier', 'a'), lemma.lemmatize('happiest', 'a'))\n",
        "print(lemma.lemmatize('fancier', 'a'), lemma.lemmatize('fanciest', 'a'))"
      ],
      "id": "1f92e781",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Jun\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "amuse amuse amuse\n",
            "happy happy\n",
            "fancy fancy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f324315"
      },
      "source": [
        "## Bag of Words - BOW"
      ],
      "id": "5f324315"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "790f0b04"
      },
      "source": [
        "### 희소 행렬 -COO 형식"
      ],
      "id": "790f0b04"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a830b1e",
        "outputId": "90be37ac-d71b-4712-cd96-ff62d4e85950"
      },
      "source": [
        "import numpy as np\n",
        "dense = np.array( [ [3, 0, 1], [0, 2, 0] ] )\n",
        "# 위의 밀집 행렬을 scipy의 coo_matrix를 이용해 COO 형식의 희소행렬 변환\n",
        "\n",
        "from scipy import sparse\n",
        "\n",
        "# 0이 아닌 데이터 추출\n",
        "data = np.array([3, 1, 2])\n",
        "\n",
        "# 행 위치와 열 위치를 각각 배열로 생성\n",
        "row_pos = np.array([0, 0, 1])\n",
        "col_pos = np.array([0, 2, 1])\n",
        "\n",
        "# sparse 패키지의 coo_matrix를 이용해 COO 형식으로 희소행렬 생성\n",
        "sparse_coo = sparse.coo_matrix((data, (row_pos, col_pos)))\n",
        "# sparse_coo는 COO형식의 희소 행렬 객체 변수 ->이를 toarray( ) 메서드를 이용해 밀집 행렬로 출력\n",
        "sparse_coo.toarray()"
      ],
      "id": "3a830b1e",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3, 0, 1],\n",
              "       [0, 2, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "948de6de"
      },
      "source": [
        "### 희소 행렬 -CSR 형식"
      ],
      "id": "948de6de"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e79b303",
        "outputId": "0ab3d9f5-3432-4017-bbf8-e60478354746"
      },
      "source": [
        "from scipy import sparse\n",
        "\n",
        "dense2 = np.array([[0, 0, 1, 0, 0, 5], \n",
        "                  [1, 4, 0, 3, 2, 5], \n",
        "                  [0, 6, 0, 3, 0, 0], \n",
        "                  [2, 0, 0, 0, 0, 0], \n",
        "                  [0, 0, 0, 7, 0, 8], \n",
        "                  [1, 0, 0, 0, 0, 0]])\n",
        "\n",
        "# 0이 아닌 데이터 추출\n",
        "data2 = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1])\n",
        "\n",
        "# 행 위치와 열 위치를 각각 array로 생성\n",
        "row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\n",
        "col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\n",
        "\n",
        "# COO형식으로 변환\n",
        "sparse_coo = sparse.coo_matrix((data2, (row_pos, col_pos)))\n",
        "\n",
        "# 행 위치 배열의 고유한 값의 시작 위치 인덱스를 배열로 생성\n",
        "row_pos_ind = np.array([0, 2, 7, 9, 10, 12, 13])\n",
        "\n",
        "# CSR 형식으로 변환\n",
        "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
        "\n",
        "print('COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
        "print(sparse_coo.toarray())\n",
        "print('CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
        "print(sparse_csr.toarray())"
      ],
      "id": "0e79b303",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
            "[[0 0 1 0 0 5]\n",
            " [1 4 0 3 2 5]\n",
            " [0 6 0 3 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [0 0 0 7 0 8]\n",
            " [1 0 0 0 0 0]]\n",
            "CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
            "[[0 0 1 0 0 5]\n",
            " [1 4 0 3 2 5]\n",
            " [0 6 0 3 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [0 0 0 7 0 8]\n",
            " [1 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78f7a5bb"
      },
      "source": [
        "dense3 = np.array([[0, 0, 1, 0, 0, 5], \n",
        "                  [1, 4, 0, 3, 2, 5], \n",
        "                  [0, 6, 0, 3, 0, 0], \n",
        "                  [2, 0, 0, 0, 0, 0], \n",
        "                  [0, 0, 0, 7, 0, 8], \n",
        "                  [1, 0, 0, 0, 0, 0]])\n",
        "\n",
        "coo = sparse.coo_matrix(dense3)\n",
        "csr = sparse.csr_matrix(dense3)"
      ],
      "id": "78f7a5bb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a06be7cb"
      },
      "source": [
        "## 텍스트 분류 실습 - 20 뉴스그룹 분류\n",
        "### 텍스트 정규화"
      ],
      "id": "a06be7cb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46d26aee",
        "outputId": "b5a90516-cda9-489e-af1e-871d5452da0b"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "news_data = fetch_20newsgroups( subset='all', random_state=156 )\n",
        "\n",
        "print(news_data.keys())"
      ],
      "id": "46d26aee",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ee37914",
        "outputId": "11767dda-ce24-4bba-dadc-548530ec6a3a"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "print('target 클래스의 값과 분포도 \\n', pd.Series(news_data.target).value_counts().sort_index())\n",
        "print('target 클래스의 이름들 \\n', news_data.target_names)"
      ],
      "id": "9ee37914",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "target 클래스의 값과 분포도 \n",
            " 0     799\n",
            "1     973\n",
            "2     985\n",
            "3     982\n",
            "4     963\n",
            "5     988\n",
            "6     975\n",
            "7     990\n",
            "8     996\n",
            "9     994\n",
            "10    999\n",
            "11    991\n",
            "12    984\n",
            "13    990\n",
            "14    987\n",
            "15    997\n",
            "16    910\n",
            "17    940\n",
            "18    775\n",
            "19    628\n",
            "dtype: int64\n",
            "target 클래스의 이름들 \n",
            " ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db9c0215",
        "outputId": "1fa55784-765d-48b5-b4fb-b192c3a386ec"
      },
      "source": [
        "print(news_data.data[0])"
      ],
      "id": "db9c0215",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From: egreen@east.sun.com (Ed Green - Pixel Cruncher)\n",
            "Subject: Re: Observation re: helmets\n",
            "Organization: Sun Microsystems, RTP, NC\n",
            "Lines: 21\n",
            "Distribution: world\n",
            "Reply-To: egreen@east.sun.com\n",
            "NNTP-Posting-Host: laser.east.sun.com\n",
            "\n",
            "In article 211353@mavenry.altcit.eskimo.com, maven@mavenry.altcit.eskimo.com (Norman Hamer) writes:\n",
            "> \n",
            "> The question for the day is re: passenger helmets, if you don't know for \n",
            ">certain who's gonna ride with you (like say you meet them at a .... church \n",
            ">meeting, yeah, that's the ticket)... What are some guidelines? Should I just \n",
            ">pick up another shoei in my size to have a backup helmet (XL), or should I \n",
            ">maybe get an inexpensive one of a smaller size to accomodate my likely \n",
            ">passenger? \n",
            "\n",
            "If your primary concern is protecting the passenger in the event of a\n",
            "crash, have him or her fitted for a helmet that is their size.  If your\n",
            "primary concern is complying with stupid helmet laws, carry a real big\n",
            "spare (you can put a big or small head in a big helmet, but not in a\n",
            "small one).\n",
            "\n",
            "---\n",
            "Ed Green, former Ninjaite |I was drinking last night with a biker,\n",
            "  Ed.Green@East.Sun.COM   |and I showed him a picture of you.  I said,\n",
            "DoD #0111  (919)460-8302  |\"Go on, get to know her, you'll like her!\"\n",
            " (The Grateful Dead) -->  |It seemed like the least I could do...\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "174eb1ba",
        "outputId": "e2ad2975-ec1b-41e8-c9eb-efd0033754f2"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# subset='train'으로 학습용 데이터만 추출, remove=('headers, 'footers'. 'quotes')로 내용만 추출\n",
        "train_news = fetch_20newsgroups(subset = 'train', remove = ('headers', 'footers', 'quotes'),\n",
        "                   random_state = 156)\n",
        "X_train = train_news.data\n",
        "y_train = train_news.target\n",
        "\n",
        "# subset='test'으로 테스트 데이터만 추출, remove=('headers', 'footers', 'quotes')로 내용만 추출\n",
        "test_news = fetch_20newsgroups(subset = 'test', remove = ('headers', 'footers', 'quotes'),\n",
        "                   random_state = 156)\n",
        "X_test = test_news.data\n",
        "y_test = test_news.target\n",
        "\n",
        "print('학습 데이터 크기 {0}, 테스트 데이터 크기 {1}'.format(len(train_news.data), len(test_news.data)))"
      ],
      "id": "174eb1ba",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "학습 데이터 크기 11314, 테스트 데이터 크기 7532\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc176c07",
        "outputId": "2b6e6630-322c-48c9-9134-d0c0efdd9e46"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Count Vectorization으로 피처 벡터화 수행\n",
        "cnt_vect = CountVectorizer()\n",
        "cnt_vect.fit(X_train)\n",
        "X_train_cnt_vect = cnt_vect.transform(X_train)\n",
        "\n",
        "# 학습 데이터로 fit( )된 CountVectorizer를 이용해 테스트 데이터를 피처 벡터화 변환 수행\n",
        "X_test_cnt_vect = cnt_vect.transform(X_test)\n",
        "\n",
        "print('학습 데이터 텍스트의 CountVectorizer Shape:', X_train_cnt_vect.shape)"
      ],
      "id": "bc176c07",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "학습 데이터 텍스트의 CountVectorizer Shape: (11314, 101631)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2905e45a",
        "outputId": "f3b1abe2-6297-4205-9b65-93ee003728b0"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# LogisiticRegression을 이용해 학습/에측/평가 수행\n",
        "lr_clf = LogisticRegression( )\n",
        "lr_clf.fit(X_train_cnt_vect, y_train)\n",
        "pred = lr_clf.predict(X_test_cnt_vect)\n",
        "print('CountVectorized Logistic Regression의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test, pred)))"
      ],
      "id": "2905e45a",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CountVectorized Logistic Regression의 예측 정확도는 0.604\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\Jun\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36d449d8",
        "outputId": "6043ad1f-31c9-41c9-e0fc-ff3920c4b52b"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# TF-IDF 벡터화를 적용해 학습 데이터 세트와 테스트 데이터 세트 변환\n",
        "tfidf_vec = TfidfVectorizer( )\n",
        "tfidf_vec.fit(X_train)\n",
        "X_train_tfidf_vect = tfidf_vec.transform(X_train)\n",
        "X_test_tfidf_vect = tfidf_vec.transform(X_test)\n",
        "\n",
        "# LogisticRegression을 이용해 학습/예측/평가 수행\n",
        "lr_clf = LogisticRegression( )\n",
        "lr_clf.fit(X_train_tfidf_vect, y_train)\n",
        "pred = lr_clf.predict(X_test_tfidf_vect)\n",
        "print('TF-IDF Logistic Regression의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test, pred)))"
      ],
      "id": "36d449d8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF-IDF Logistic Regression의 예측 정확도는 0.674\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "696631fa",
        "outputId": "4f4468a4-7f71-4e65-ac95-ef9ee2cf4bcb"
      },
      "source": [
        "# stop words 필터링을 추가하고 ngram을 기본 (1, 1)에서 (1, 2)로 변경해 피처 벡터화 적용\n",
        "tfidf_vect = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_df=300)\n",
        "tfidf_vect.fit(X_train)\n",
        "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
        "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
        "\n",
        "lr_clf = LogisticRegression()\n",
        "lr_clf.fit(X_train_tfidf_vect, y_train)\n",
        "pred = lr_clf.predict(X_test_tfidf_vect)\n",
        "\n",
        "print('TF-IDF Vectorized Logistic Regression 의 예측 정확도는{0:.3f}'.format(accuracy_score(y_test, pred)))"
      ],
      "id": "696631fa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF-IDF Vectorized Logistic Regression 의 예측 정확도는0.692\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a743057",
        "outputId": "d93466d3-788d-44aa-9457-08540ca4c420"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# 최적 C 값 도출 튜닝 수행. CV는 3 폴드 세트로 설정\n",
        "params = {'C': [0.01, 0.1, 1, 5, 10]}\n",
        "grid_cv_lr = GridSearchCV(lr_clf, param_grid=params, cv=3, scoring='accuracy', verbose=1)\n",
        "grid_cv_lr.fit(X_train_tfidf_vect, y_train)\n",
        "print('Logistic Regression best C parameter: ', grid_cv_lr.best_params_)\n",
        "\n",
        "# 최적 C 값으로 학습된 grid_cv로 예측 및 정확도 평가\n",
        "pred = grid_cv_lr.predict(X_test_tfidf_vect)\n",
        "print('TF-IDF Vectorized Logistic Regression의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test, pred)))"
      ],
      "id": "7a743057",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\Jun\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "C:\\Users\\Jun\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "C:\\Users\\Jun\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "C:\\Users\\Jun\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "C:\\Users\\Jun\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "C:\\Users\\Jun\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "C:\\Users\\Jun\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression best C parameter:  {'C': 10}\n",
            "TF-IDF Vectorized Logistic Regression의 예측 정확도는 0.701\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9090706"
      },
      "source": [
        ""
      ],
      "id": "c9090706",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae0a6d98"
      },
      "source": [
        ""
      ],
      "id": "ae0a6d98",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d0b609d"
      },
      "source": [
        ""
      ],
      "id": "7d0b609d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff046b05"
      },
      "source": [
        ""
      ],
      "id": "ff046b05",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fe0cdc3"
      },
      "source": [
        ""
      ],
      "id": "8fe0cdc3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8d2760c"
      },
      "source": [
        ""
      ],
      "id": "e8d2760c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67aa6741"
      },
      "source": [
        ""
      ],
      "id": "67aa6741",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "161df1be"
      },
      "source": [
        ""
      ],
      "id": "161df1be",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feeda9e4"
      },
      "source": [
        ""
      ],
      "id": "feeda9e4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1e88930"
      },
      "source": [
        ""
      ],
      "id": "d1e88930",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb745a75"
      },
      "source": [
        ""
      ],
      "id": "fb745a75",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "843b53d8"
      },
      "source": [
        ""
      ],
      "id": "843b53d8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb9d9fc3"
      },
      "source": [
        ""
      ],
      "id": "bb9d9fc3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "097a2635"
      },
      "source": [
        ""
      ],
      "id": "097a2635",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2c6c18f"
      },
      "source": [
        ""
      ],
      "id": "b2c6c18f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38edf25e"
      },
      "source": [
        ""
      ],
      "id": "38edf25e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5a98fe0"
      },
      "source": [
        ""
      ],
      "id": "d5a98fe0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b12ad4f"
      },
      "source": [
        ""
      ],
      "id": "0b12ad4f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16dc2fb5"
      },
      "source": [
        ""
      ],
      "id": "16dc2fb5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fffc783"
      },
      "source": [
        ""
      ],
      "id": "4fffc783",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb887dc9"
      },
      "source": [
        ""
      ],
      "id": "cb887dc9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "612e8777"
      },
      "source": [
        ""
      ],
      "id": "612e8777",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4d38872"
      },
      "source": [
        ""
      ],
      "id": "c4d38872",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9f163fe"
      },
      "source": [
        ""
      ],
      "id": "d9f163fe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39d07e53"
      },
      "source": [
        ""
      ],
      "id": "39d07e53",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d8312f3"
      },
      "source": [
        ""
      ],
      "id": "1d8312f3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44d31589"
      },
      "source": [
        ""
      ],
      "id": "44d31589",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ae41f0d"
      },
      "source": [
        ""
      ],
      "id": "4ae41f0d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f408c0f"
      },
      "source": [
        ""
      ],
      "id": "2f408c0f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1526dd3"
      },
      "source": [
        ""
      ],
      "id": "d1526dd3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbab49b6"
      },
      "source": [
        ""
      ],
      "id": "dbab49b6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "240f3272"
      },
      "source": [
        ""
      ],
      "id": "240f3272",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e021ab74"
      },
      "source": [
        ""
      ],
      "id": "e021ab74",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "030afed9"
      },
      "source": [
        ""
      ],
      "id": "030afed9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e35b201"
      },
      "source": [
        ""
      ],
      "id": "4e35b201",
      "execution_count": null,
      "outputs": []
    }
  ]
}