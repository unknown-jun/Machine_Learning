{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5be754b8",
   "metadata": {},
   "source": [
    "# 앙상블 개요\n",
    "- 앙상블 학습(Ensemble Learning)을 통한 분류는 여러 개의 분류기(Classifier)를 생성하고 그 예측을 결합함으로써 보다 정확한 최종 예측을 도출하는 기법\n",
    "- 앙상블 학습의 핵심은 **여러 개의 약 분류기** (Weak Classifier)를 결합하여 **강 분류기**(Strong Classifier)를 만드는 것이다.\n",
    "- 앙상블 학습의 유형은 보팅(Voting), 배깅(Bagging), 부스팅(Boosting)의 세가지로 나눌 수 있고 이 외에 스태킹도 있음\n",
    "    - 약 분류기를 병렬적으로 사용하면 배깅\n",
    "    - Variance(데이터 밀집도)를 감소시키고 싶다면 배깅\n",
    "    - 약 분류기를 순차적으로 사용하면 부스팅\n",
    "    - Bias(중심과의 거리)를 감소시키고 싶다면 부스팅\n",
    "- 보팅의 경우 서로 다른 알고리즘을 가진 분류기를 결합\n",
    "- 배깅의 경우 각각의 분류기가 모드 같은 유형의 알고리즘 기반이지만, 데이터 샘플링을 서로 다르게 가져가면서 학습을 수행해 보팅을 함\n",
    "- 부스팅의 경우 여러 개의 분류기가 학습-예측하면서 잘못 예측한 데이터에 가중치 부여를 통해 오류를 개선해 나가는 학습 방법\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/70187490/130563710-86a48e34-21e1-4aef-85f1-2df7732b49ec.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f0bbf5",
   "metadata": {},
   "source": [
    "# 보팅\n",
    "![image](https://user-images.githubusercontent.com/70187490/130564309-a4df1819-0af1-4c68-bf78-b61844e4c6c4.png)\n",
    "- 보팅에는 두가지 방법이 있음\n",
    "1. 하드 보팅\n",
    "    - 하드 보팅을 이용한 분류는 다수결 원칙과 비슷함\n",
    "    - 예측한 결괏값들 중 다수의 분류기가 결정한 예측값을 최종 보팅 결괏값으로 선정하는 것\n",
    "2. 소프트 보팅\n",
    "    - 분류기들의 레이블 값 결정 확률을 모두 더하고 이를 평균해서 이들 중 확률이 가장 높은 레이블 값을 최종 보팅 결괏값으로 선정\n",
    "- 일반적으로 소프트 보팅을 적용"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b57a74b",
   "metadata": {},
   "source": [
    "# 배깅\n",
    "- 원자료로부터 Random Sampling을 통해 크기가 동일한 n개의 표본자료를 생성하고 (부트 스트래핑 방식을 주로 사용)\n",
    "- 각 표본 자료를 병렬적으로 모델링한 후 나온 예측 변수들을 결합하여 최종 모형을 생성하는 것  \n",
    "    - 이러한 각 샘플의 예측변수들을 결합하는 방법은 목표 변수가 연속형일 때는 평균(Average), 범주형일 때는 다중 투표(Majority vote)를 사용하는 것이 일반적  \n",
    "- 배깅은 예측 모형의 변동성이 큰 경우(High Variance) 예측 모형의 변동성을 감소시키기 위해 사용됨\n",
    "    - 즉, 원자료로부터 여러 번의 복원 샘플링을 통해 예측 모형의 분산을 줄여 줌으로써 예측력을 향상 시키는 방법\n",
    "    - 따라서 배깅은 일반적으로 과대적합된 모형, 편의가 작고 분산이 큰 모형에 사용하는 것이 적합함\n",
    "- 대표적인 모형으로 'Random Forest'가 있다.\n",
    "\n",
    "\n",
    "\\* 부트스트래핑(bootstrapping)이란\n",
    "![image](https://user-images.githubusercontent.com/70187490/130565112-4620c8af-c569-4c1c-94b8-423d751ace5d.png)\n",
    "    - 개별 트리가 학습하는 데이터 세트를 전체 데이터에서 일부가 중첩되게 샘플링  \n",
    "    - 서브 데이터의 데이터 건수는 전체 데이터 건수와 동일하지만, 개별 데이터가 중첩되어 만들어 짐\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb07990",
   "metadata": {},
   "source": [
    "# 부스팅\n",
    "- 부스팅 알고리즘은 여러 개의 약한 학습기(Weak learner)를 순차적으로 학습-예측하면서 잘못 예측한 데이터에 가중치 부여를 통해 오류를 개선해 나가면서 학습하는 방식\n",
    "    - 잘못 분류된 개체들에 집중하여 새로운 분류규칙을 만드는 단계를 반복하는 방법\n",
    "    - 약한 예측모형들의 학습 에러에 가중치를 두고 순차적으로 다음 학습 모델에 반영하여 강한 예측 모형을 만드는 것\n",
    "    - 처음 원자료의 객체들에는 동일한 가중치에서 시작하지만, 모델링을 통한 예측변수에 의해 오분류된 객체들에게는 높은 가중치를 부여하고, 정분류된 객체들에게는 낮은 가중치를 부여하여 오분류된 객체들이 더 잘 분류되도록 하는 방법\n",
    "- 즉, 부스팅 방법은 예측모형의 편차가 큰 경우(High Bias) 편차를 줄이기 위해 사용\n",
    "- 대표적인 부스팅 기반 모델은  'AdaBoost','CatBoost','Gradient Boost','XGBoost' 등이 있다.\n",
    "\n",
    "<br>\n",
    "\n",
    "- 부스팅 학습 과정(에이다 부스트)\n",
    "![image](https://user-images.githubusercontent.com/70187490/130567290-3e56b4aa-7b9f-4ba0-8069-46434fe41e54.png)\n",
    "\n",
    "- step 1은 첫 번째 약한 학습기(Weak learner)가 분류 기준 1로 +와 -를 분류한 것. 동그라미로 표시된 데이터는 ⊕ 데이터가 잘못 분류된 오류 데이터\n",
    "- step 2에서는 이 오류 데이터에 대해서 가중치 값을 부여한 것. 가중치가 보유된 오류 +데이터는 다음 약한 학습기가 더 잘 분류할 수 있게 크기가 커짐\n",
    "- step 3은 두 번째 약한 학습기가 분류 기준로 +와 -를 분류한 것. 마찬가지로 동그라미로 표시된 ⊖ 데이터는 잘못 분류된 오류 데이터\n",
    "- step 4은 잘못 분류된 이 - 오류 데이터에 대해 다음 약한 학습기가 잘 분류할 수 있게 더 큰 가중치를 부여.(오류 -데이터의 크기가 커짐)\n",
    "- step 5은 세번째 약한 학습기가 분류 기준 3으로 +와 -를 분류하고 오류 데이터를 찾음. 이렇게 약한 학습기가 순차적으로 순차적으로 오류 값에 대해 가중치를 부여한 예측 결정 기준을 모두 결합해 예측을 결정"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
